---
title: "Class Exercise 2"
author: "Sergei Korotkikh, Vadim Maletskii"
date: "06/12/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# This is an overview of our workflow of the Class Exercise 2

## Task 1

There is no *robot.txt* (error 404), which means there are no stated rules for scraping pages from this root. However, since we should be nice and polite people, while scraping we will identify ourselves and put *sys.sleep* function for big loops.

## Tasks 2 and 3
We started with loading packages that we need. Then we loaded the page, identifying who we are (putting email and info about our R version). After that with the help of *XML::getHTMLLinks* function we got HTMl links and then used regexp to filter them. After filtering we ended up with 28 links, but not all of them were redirecting to other posts of the "beppegrillo.it blog".
It was pointless to further filter with the help of regexps because the list consists of only 28 links and it is to hard to create a search pattern as there are some links with just names and they again should be subset manually even with regexps. For example : "https://beppegrillo.it/cookie-policy/" or "https://beppegrillo.it/grilloteca/" (which actually is a part of "categories" of the website). That is why we created a data frame and kept manually the links for other blog posts.

```{r}
# Loading packages 
library(tidyverse)
library(stringr)

# Loading the page 
rcurl_url <- 'https://beppegrillo.it/un-mare-di-plastica-ci-sommergera/'
email <- "wiscain@yandex.ru"
agent <- R.Version()$version.string
user_id <- c(From = email, `User-Agent` = agent)
rcurl_page <- RCurl::getURL(url = rcurl_url,  httpheader= (user_id))

# Getting the links 
links <- XML::getHTMLLinks(rcurl_page)
links_filtered <- stringr::str_subset(links, pattern = '^https://beppegrillo\\.it[[:graph:]]') %>% unique()

# Final filtering

links_filtered_2 = as_data_frame(links_filtered)
links_filtered_final = links_filtered_2[c(18,19,21,22,23,24,25,26,27), ]

```

## Task 4 

```{r}
# Loading packages ----
library(httr)
library(rvest)

# Putting pages into a list ----
archive_url <- 'https://beppegrillo.it/category/archivio/2016/'
archive_links <- vector(mode = 'list', length = 47)
for (i in 1:47) {
  link <- str_c('https://beppegrillo.it/category/archivio/2016/', 'page/', i)
  page <- read_html(link)
  archive_links[[i]] <- html_attr(html_nodes(page, '.td_module_10 a'), 'href') %>% unique()
}

# Downloading the pages ----
counter = 1  # it is used to save the page with its own number
for (i in 1:47) {
  for (j in 1:length(archive_links[[i]])) {
    grillo_page <- RCurl::getURL(url = archive_links[[i]][j],  httpheader= (user_id))
    download.file(url = archive_links[[i]][j], destfile = here::here('pages', str_c('page_', counter, '.html')))
    counter = counter + 1
    Sys.sleep(2)
  }
}

# Extracting the main text from the pages ----
main_texts <- vector(mode = 'list', length = 470)
for (i in 1:470) {
  main_texts[[i]] <- rvest::read_html(here::here('pages', str_c('page_', i, '.html'))) %>% 
    html_elements(css = "p") %>% 
    html_text()
}

```

## Task 5 

