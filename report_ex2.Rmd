---
title: "Class Exercise 2"
author: "Sergei Korotkikh, Vadim Maletskii"
date: "06/12/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# This is an overview of our workflow of the Class Exercise 2

## Task 1

There is no *robot.txt* (error 404), which means there are no stated rules for scraping pages from this root. However, since we should be nice and polite people, while scraping we will identify ourselves and put *Sys.sleep* function for big loops.

## Tasks 2 and 3

We started with loading packages that we need. Then we loaded the page, identifying who we are (putting email and info about our R version). After that with the help of *XML::getHTMLLinks* function we got HTML links and then used regexp to filter them. After filtering we ended up with 28 links, but not all of them were redirecting to other posts of the "beppegrillo.it blog". It was pointless to further filter with the help of regexps because the list consists of only 28 links and it is to hard to create a search pattern as there are some links with just names and they again should be subset manually even with regexps. For example : "<https://beppegrillo.it/cookie-policy/>" or "<https://beppegrillo.it/grilloteca/>" (which actually is a part of "categories" of the website). That is why we created a data frame and kept manually the links for other blog posts.

```{r eval=FALSE, results= "hide"}
# Loading packages 
library(tidyverse)
library(stringr)

# Loading the page 
rcurl_url <- 'https://beppegrillo.it/un-mare-di-plastica-ci-sommergera/'
email <- "wiscain@yandex.ru"
agent <- R.Version()$version.string
user_id <- c(From = email, `User-Agent` = agent)
rcurl_page <- RCurl::getURL(url = rcurl_url,  httpheader= (user_id))

# Getting the links 
links <- XML::getHTMLLinks(rcurl_page)
links_filtered <- stringr::str_subset(links, pattern = '^https://beppegrillo\\.it[[:graph:]]') %>% unique()

# Final filtering

links_filtered_2 = as_data_frame(links_filtered)
links_filtered_final = links_filtered_2[c(18,19,21,22,23,24,25,26,27), ]

```

## Task 4

Firstly, for each of the 47 pages from "<https://beppegrillo.it/category/archivio/2016/>", we got all the links and placed them into a character vector. Then, for each single linked blog post, we downloaded the page as a file, revealing our identity and putting *Sys.sleep* of 2 seconds to be polite. After that we used "SelectorGadget" to find out the path for the main text and then scraped it. When there was no text, we got " " as a result.

```{r eval=FALSE, results= "hide"}
# Loading packages
library(httr)
library(rvest)

# Putting pages into a list
archive_url <- 'https://beppegrillo.it/category/archivio/2016/'
archive_links <- vector(mode = 'list', length = 47)
for (i in 1:47) {
  link <- str_c('https://beppegrillo.it/category/archivio/2016/', 'page/', i)
  page <- read_html(link)
  archive_links[[i]] <- html_attr(html_nodes(page, '.td_module_10 a'), 'href') %>% unique()
}

# Downloading the pages
counter = 1  # it is used to save the page with its own number
for (i in 1:47) {
  for (j in 1:length(archive_links[[i]])) {
    grillo_page <- RCurl::getURL(url = archive_links[[i]][j],  httpheader= (user_id))
    download.file(url = archive_links[[i]][j], destfile = here::here('pages', str_c('page_', counter, '.html')))
    counter = counter + 1
    Sys.sleep(2)
  }
}

# Extracting the main text from the pages
main_texts <- vector(mode = 'list', length = 470)
for (i in 1:470) {
  main_texts[[i]] <- rvest::read_html(here::here('pages', str_c('page_', i, '.html'))) %>% 
    html_elements(css = "p") %>% 
    html_text()
}

```

## Task 5

To "crawl" means to automatically browse and download (1) web pages, and (2) web pages from links, contained in the (1) page, and (3) web pages from links contained in the (2) pages, and so on, with the help of special programs called "web spider" or "web crawler". There are several types of web spiders (or crawlers), for instance universal spiders scrape web pages, regardless of their content, while preferential spiders can be more precise. This method allows to collect really huge amount of data with quite simple coding.

Basically, what we did manually in task 4 step by step could have been done automatically with a single command if we had used "Rcrawler".

We think that for building a spider scraper we could use *Rcrawler()* function with arguments:\
*Website* (to set the root URL of the website)\
*Useragent* (to be polite and reveal identity)\
*crawlUrlfilter* (to specify the exact site to be scraped (and not everything from the root)\
*crawlZoneCSSPat* (to scrape main texts from the pages)\
*DIR* (to specify where to save collected data)\
*RequestsDelay* (to set a pause between requests to be polite)
